{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vasanth/programming/690N/project/conversational_agent_personified/data/glove.twitter.27B/glove.twitter.27B.200d.txt\n",
      "/home/vasanth/programming/690N/project/conversational_agent_personified/data/glove.twitter.27B\n"
     ]
    }
   ],
   "source": [
    "root_dir = os.path.abspath('../')\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "res_dir = os.path.join(root_dir, 'res')\n",
    "res_filtered_dir = os.path.join(res_dir, 'filtered')\n",
    "res_raw_dir = os.path.join(res_dir, 'raw')\n",
    "res_raw_vocab = os.path.join(res_raw_dir, 'vocab')\n",
    "res_raw_wti = os.path.join(res_raw_dir, 'wti')\n",
    "res_raw_embeddings = os.path.join(res_raw_dir, 'embed')\n",
    "res_filt_vocab = os.path.join(res_filtered_dir, 'vocab')\n",
    "res_filt_wti = os.path.join(res_filtered_dir, 'wti')\n",
    "res_filt_embeddings = os.path.join(res_filtered_dir, 'embed')\n",
    "raw_data_dir = os.path.join(data_dir,'raw_data')\n",
    "clean_data_dir = os.path.join(data_dir, 'clean_data')\n",
    "embeddings_dir = os.path.join(data_dir, 'glove.twitter.27B')\n",
    "embeddings_file = os.path.join(embeddings_dir, 'glove.twitter.27B.200d.txt')\n",
    "print embeddings_file\n",
    "print embeddings_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_if_possible(word):\n",
    "    word = word.lower()\n",
    "    word = stemmer.stem(word)\n",
    "    return word\n",
    "        \n",
    "def construct_vocabulary():\n",
    "    words = []\n",
    "    res_word_to_index = []\n",
    "    res_embeddings = []\n",
    "    print \"Extract vocabulary...\"\n",
    "    files_processed = 0\n",
    "    for fname in glob.iglob(res_dir + '/*_ascii.txt'):\n",
    "        print fname\n",
    "        files_processed += 1\n",
    "        for line in open(fname):\n",
    "            words += [stem_if_possible(w) for w in list(set(wordpunct_tokenize(line)))]\n",
    "    print \"Files processed {0}\".format(files_processed)\n",
    "    print len(list(set(words)))\n",
    "    print \"Now read from twitter embeddings and update\"\n",
    "    word_list = list(set(words))\n",
    "    for word in word_list:\n",
    "        for fname in glob.iglob(res_raw_wti + '/*.pickle'):\n",
    "            vocab_partial = pickle.load(open(fname,\"rb\"))\n",
    "            vocab_partial = [stem_if_possible(v) for v in vocab_partial]\n",
    "            if word in vocab_partial:\n",
    "                file_index = int(filter(str.isdigit, fname.split(\"/\")[-1]))\n",
    "                embed_file = \"embedding\"+str(file_index)+\".npy\"\n",
    "                embed_file_full = os.path.join(res_raw_embeddings,embed_file)\n",
    "                embed_partial = np.load(embed_file_full)\n",
    "                print \"Word: {0} index {1}\".format(word, file_index)\n",
    "                res_word_to_index.append(word)\n",
    "                res_embeddings.append(embed_partial[vocab_partial[word]])\n",
    "    print \"Done with vocab extraction\"\n",
    "    return word_list\n",
    "\n",
    "import re\n",
    "import string\n",
    "printable = set(string.printable)\n",
    "func = lambda s: filter(lambda x: x in printable, s)\n",
    "\n",
    "def write_our_words():\n",
    "    words = []\n",
    "    fnames = [res_dir + '/input1.txt', res_dir + '/input2.txt']\n",
    "    for fname in fnames:\n",
    "        print fname\n",
    "        for line in open(fname):\n",
    "#             line = re.sub(r'(\\.)+', '.', line)\n",
    "#             line = re.sub(r'(\\?)+', '?', line)\n",
    "#             line = re.sub(r'(\\')+', '\\'', line)\n",
    "#             line = re.sub(r'(\\!)+', '!', line)\n",
    "#             line = re.sub('(?<! )(?=[.,!?()])|(?<=[.,!?()])(?! )', r'', line)\n",
    "#             words += [func(w) for w in line.lower().split()]\n",
    "            line = line.translate(None, string.punctuation)\n",
    "            print line\n",
    "            words += [func(w) for w in line.lower().split()]\n",
    "    with open('vocab.txt', 'w') as f:\n",
    "        f.write('\\n'.join(list(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vasanth/programming/690N/project/conversational_agent_personified/res/input1.txt\n",
      "/home/vasanth/programming/690N/project/conversational_agent_personified/res/input2.txt\n"
     ]
    }
   ],
   "source": [
    "write_our_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract vocabulary...\n",
      "/home/vasanth/programming/690N/project/conversational_agent_personified/res/input3_ascii.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe0 in position 2: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3ed1df2ad6ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-9af12f411d5a>\u001b[0m in \u001b[0;36mconstruct_vocabulary\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfiles_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mwords\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstem_if_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordpunct_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Files processed {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9af12f411d5a>\u001b[0m in \u001b[0;36mstem_if_possible\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstem_if_possible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/stem/porter.pyc\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/nltk/stem/porter.pyc\u001b[0m in \u001b[0;36m_step1a\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# that 'flies'->'fli' but 'dies'->'die' etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLTK_EXTENSIONS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ies'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ies'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ie'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe0 in position 2: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "cv = construct_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def question_answer_format():\n",
    "    vocabulary = {}\n",
    "    if not os.path.exists(res_raw_wti):\n",
    "        os.makedirs(res_raw_wti)\n",
    "    if not os.path.exists(res_raw_wti):\n",
    "        os.makedirs(res_raw_wti)\n",
    "    questions_file = os.path.join(res_dir,'input1.txt')\n",
    "    answers_file = os.path.join(res_dir,'input2.txt')\n",
    "    questions_file1 = os.path.join(res_dir,'input3.txt')\n",
    "    answers_file1 = os.path.join(res_dir,'input4.txt')    \n",
    "    print \"Start writing ques answer files...\"\n",
    "    import glob\n",
    "    lines_written = 0\n",
    "    files_processed = 0\n",
    "    for full_filename in glob.iglob(clean_data_dir + '/*/*.txt'):\n",
    "        files_processed += 1\n",
    "        with open(full_filename) as f1:\n",
    "            lines = f1.readlines()\n",
    "            with open(questions_file, 'a') as f2, open(answers_file, 'a') as f3, open(questions_file1, 'a') as f4, open(answers_file1, 'a') as f5:                            \n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                        line = line.split(\":\")[1]\n",
    "                        line1 = re.sub(r'\\([^)]*\\)', '', line).strip(\" \")\n",
    "                        f2.write(line)\n",
    "                        f4.write(line1)\n",
    "                        if lines_written == 0:\n",
    "                            pass          \n",
    "                        elif lines_written % 100000 == 0:\n",
    "                            f3.write(line)\n",
    "                            f5.write(line1)\n",
    "                            print \"Lines written: {0}\".format(lines_written)\n",
    "                            print \"Writing from {0}\".format(full_filename.split(\"/\")[-1])\n",
    "                        else:\n",
    "                            f3.write(line)\n",
    "                            f5.write(line1)\n",
    "                        lines_written += 1\n",
    "    print \"Finished writing all files.\"\n",
    "    print \"{0} files processed. {1} lines written\".format(files_processed, lines_written)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing ques answer files...\n",
      "Lines written: 100000\n",
      "Writing from Ordinary People.txt\n",
      "Lines written: 200000\n",
      "Writing from My Mother Dreams the Satan's Disciples in New York.txt\n",
      "Lines written: 300000\n",
      "Writing from State and Main.txt\n",
      "Lines written: 400000\n",
      "Writing from Garden State.txt\n",
      "Lines written: 500000\n",
      "Writing from Precious.txt\n",
      "Lines written: 600000\n",
      "Writing from Being There.txt\n",
      "Lines written: 700000\n",
      "Writing from Analyze That.txt\n",
      "Lines written: 800000\n",
      "Writing from Two For The Money.txt\n",
      "Finished writing all files.\n",
      "1052 files processed. 839983 lines written\n"
     ]
    }
   ],
   "source": [
    "question_answer_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadWordEmbeddings(filename):\n",
    "    #vocabulary = []\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    i = 0\n",
    "    file_point = 0\n",
    "    if not os.path.exists(res_raw_embeddings):\n",
    "        os.makedirs(res_raw_embeddings)\n",
    "    if not os.path.exists(res_raw_wti):\n",
    "        os.makedirs(res_raw_wti)\n",
    "    for line in open(filename,'r'):\n",
    "        one_record = line.strip().split(\" \")\n",
    "        #vocabulary.append(one_record[0])\n",
    "        word_to_index[one_record[0]] = i%500000\n",
    "        embeddings.append(one_record[1:])\n",
    "        i += 1\n",
    "        if i%500000 == 0:\n",
    "            print \"Embedding of {0} words done. Current token: {1}, isCapital : {2}\".format(i, one_record[0], one_record[0].islower())\n",
    "            print \"Partial reading twitter glove vocab done. |V| = {0}, |embed| = {1}\".format(len(word_to_index),len(embeddings))\n",
    "            \n",
    "            print \"Now pickling word_to_index dict...\"\n",
    "            wti = \"wti\"+str(file_point)+\".pickle\"\n",
    "            embed = \"embedding\"+str(file_point)+\".npy\"\n",
    "            wti_filename = os.path.join(res_raw_wti, wti)\n",
    "            embed_filename = os.path.join(res_raw_embeddings, embed)\n",
    "            with open(wti_filename, 'wb') as handle:\n",
    "                pickle.dump(word_to_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print \"Pickling word_to_index dict complete.\"\n",
    "            print \"Now pickling embeddings array...\"\n",
    "            np.save(embed_filename, embeddings, allow_pickle=True)\n",
    "            print \"Pickling embeddings array complete.\"                \n",
    "            print \"Loaded embeddings!!\"            \n",
    "            word_to_index = {}\n",
    "            embeddings = []\n",
    "            file_point += 1\n",
    "    print \"Embedding of {0} words done. Current token: {1}, isCapital : {2}\".format(i, one_record[0], one_record[0].islower())\n",
    "    print \"Partial reading twitter glove vocab done. |V| = {0}, |embed| = {1}\".format(len(word_to_index),len(embeddings))\n",
    "\n",
    "    print \"Now pickling word_to_index dict...\"\n",
    "    wti = \"wti\"+str(file_point)+\".pickle\"\n",
    "    embed = \"embedding\"+str(file_point)+\".npy\"\n",
    "    wti_filename = os.path.join(res_raw_wti, wti)\n",
    "    embed_filename = os.path.join(res_raw_embeddings, embed)\n",
    "    with open(wti_filename, 'wb') as handle:\n",
    "        pickle.dump(word_to_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print \"Pickling word_to_index dict complete.\"\n",
    "    print \"Now pickling embeddings array...\"\n",
    "    np.save(embed_filename, embeddings, allow_pickle=True)\n",
    "    print \"Pickling embeddings array complete.\"                \n",
    "    print \"Loaded embeddings!!\"            \n",
    "    print \"Finished reading twitter glove vocab\"\n",
    "    #return word_to_index, embeddings\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of 500000 words done. Current token: تغيّروُآ, isCapital : False\n",
      "Partial reading twitter glove vocab done. |V| = 500000, |embed| = 500000\n",
      "Now pickling word_to_index dict...\n",
      "Pickling word_to_index dict complete.\n",
      "Now pickling embeddings array...\n",
      "Pickling embeddings array complete.\n",
      "Loaded embeddings!!\n",
      "Embedding of 1000000 words done. Current token: ikatannya, isCapital : True\n",
      "Partial reading twitter glove vocab done. |V| = 500000, |embed| = 500000\n",
      "Now pickling word_to_index dict...\n",
      "Pickling word_to_index dict complete.\n",
      "Now pickling embeddings array...\n",
      "Pickling embeddings array complete.\n",
      "Loaded embeddings!!\n",
      "Embedding of 1193517 words done. Current token: <unk>, isCapital : True\n",
      "Partial reading twitter glove vocab done. |V| = 193515, |embed| = 193517\n",
      "Now pickling word_to_index dict...\n",
      "Pickling word_to_index dict complete.\n",
      "Now pickling embeddings array...\n",
      "Pickling embeddings array complete.\n",
      "Loaded embeddings!!\n",
      "Finished reading twitter glove vocab\n"
     ]
    }
   ],
   "source": [
    "embd_file_dir = os.path.join(data_dir, 'glove.twitter.27B')\n",
    "embd_file_name = os.path.join(embd_file_dir,'glove.twitter.27B.200d.txt')\n",
    "loadWordEmbeddings(embd_file_name)\n",
    "#print word_to_index\n",
    "#print embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start filtering word embeddings...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80444"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fawn',\n",
       " 'Sergeantthis',\n",
       " 'MargaretI',\n",
       " 'HURTLE',\n",
       " 'Pecrowsky',\n",
       " 'mussss',\n",
       " 'nunnery',\n",
       " 'EXPLAIN',\n",
       " 'Dago',\n",
       " 'INTRUDERS',\n",
       " 'Pront',\n",
       " 'Popscile',\n",
       " 'Plasticine',\n",
       " 'teleflorists',\n",
       " 'woods',\n",
       " 'clotted',\n",
       " 'spiders',\n",
       " 'hanging',\n",
       " 'woody',\n",
       " 'somepla',\n",
       " 'Jaundice',\n",
       " 'comically',\n",
       " 'localized',\n",
       " 'spidery',\n",
       " 'crasait',\n",
       " 'disobeying',\n",
       " 'caner',\n",
       " 'canes',\n",
       " 'LAST',\n",
       " 'ition',\n",
       " 'ChristDecember',\n",
       " 'chatter',\n",
       " 'OPAL',\n",
       " 'Craziness',\n",
       " 'sociopathy',\n",
       " 'e_',\n",
       " 'Mmrrunm',\n",
       " 'lency',\n",
       " 'DISHEARTENED',\n",
       " 'Vengeance',\n",
       " 'caned',\n",
       " 'Western',\n",
       " 'Writerscards',\n",
       " 'crossbar',\n",
       " 'LASH',\n",
       " '_whole_',\n",
       " 'isMy',\n",
       " 'Retreat',\n",
       " 'Euro',\n",
       " 'Tootie',\n",
       " 'Debts',\n",
       " 'Mortes',\n",
       " 'Waziri',\n",
       " 'savaient',\n",
       " 'WINNIE',\n",
       " 'Valle',\n",
       " 'Famed',\n",
       " 'Blade',\n",
       " 'itive',\n",
       " 'GALLARDO',\n",
       " 'pigment',\n",
       " 'kid\\xc2',\n",
       " 'kid\\xc3',\n",
       " 'WATERMELON',\n",
       " 'Guvner',\n",
       " 'rying',\n",
       " 'PORTLAND',\n",
       " 'Booteria',\n",
       " 'screaming',\n",
       " 'FAVs',\n",
       " 'prizing',\n",
       " 'wooded',\n",
       " 'inseminoid',\n",
       " 'Reconcile',\n",
       " 'Baaaaaaa',\n",
       " 'wooden',\n",
       " 'SULTAN',\n",
       " 'Sergeants',\n",
       " 'Sack',\n",
       " 'viable',\n",
       " 'circuitry',\n",
       " 'crotch',\n",
       " 'WINCE',\n",
       " 'WINCH',\n",
       " 'shows',\n",
       " 'HAPPENED',\n",
       " 'chaussure',\n",
       " 'Pinkerton',\n",
       " 'pizzle',\n",
       " 'inevitably',\n",
       " 'waitsQuelle',\n",
       " 'LAVGH',\n",
       " 'snuggles',\n",
       " 'complainers',\n",
       " 'blabby',\n",
       " 'Shocked',\n",
       " 'Sponsorship',\n",
       " '270',\n",
       " '271',\n",
       " '272',\n",
       " '273',\n",
       " 'tunpuna',\n",
       " '275',\n",
       " 'imploding',\n",
       " '278',\n",
       " 'Honorable',\n",
       " 'snuggled',\n",
       " 'softlyWe',\n",
       " 'inanimate',\n",
       " 'linga',\n",
       " 'errors',\n",
       " 'RADIOS',\n",
       " 'attendant335',\n",
       " 'Initially',\n",
       " 'aredraped',\n",
       " 'REFUGEES',\n",
       " 'VALERIUS',\n",
       " 'consulat',\n",
       " 'Hamilton',\n",
       " 'endCy',\n",
       " 'chordOf',\n",
       " 'designing',\n",
       " 'numeral',\n",
       " 'pawed',\n",
       " 'STAMMERS',\n",
       " 'BIRD',\n",
       " 'TRAP',\n",
       " 'Steinway',\n",
       " 'Heinel',\n",
       " 'Transat',\n",
       " 'Estimable',\n",
       " 'Timbers',\n",
       " 'Happening',\n",
       " 'Bahenie',\n",
       " 'ConcannonHis',\n",
       " 'mailings',\n",
       " 'gunleather',\n",
       " 'meshuaena',\n",
       " 'brainwashed',\n",
       " 'affiliates',\n",
       " 'steech',\n",
       " 'Orderly',\n",
       " 'perfunctorily',\n",
       " 'Crewman',\n",
       " 'Meringue',\n",
       " 'finked',\n",
       " 'affiliated',\n",
       " 'CLARINET',\n",
       " 'outWhy',\n",
       " 'Colated',\n",
       " 'kidO',\n",
       " '27A',\n",
       " '27B',\n",
       " 'kids',\n",
       " 'uplifting',\n",
       " 'Downfall',\n",
       " 'MATCHES',\n",
       " 'woogie',\n",
       " 'kidz',\n",
       " 'deferring',\n",
       " 'Feathers',\n",
       " 'Bobsledder',\n",
       " 'controversy',\n",
       " '27P',\n",
       " 'grinThis',\n",
       " 'kidd',\n",
       " 'Dislikes',\n",
       " 'CULOBAI',\n",
       " 'Shelbourne',\n",
       " 'neurologist',\n",
       " 'chuckies',\n",
       " 'spotty',\n",
       " 'LAUGHTER',\n",
       " 'Bullfighting',\n",
       " 'Isabelle',\n",
       " 'rebel',\n",
       " 'cobblers',\n",
       " 'topography',\n",
       " 'projection',\n",
       " '_knowledge_',\n",
       " 'Harvey',\n",
       " 'ZZZZING',\n",
       " 'Revolver',\n",
       " 'SMITHS',\n",
       " 'saysLeonard',\n",
       " 'getI',\n",
       " 'INSERTOh',\n",
       " 'Urghh',\n",
       " 'unsinkable',\n",
       " 'stern',\n",
       " 'Detector',\n",
       " 'liveryman',\n",
       " 'Stank',\n",
       " 'inscribeAs',\n",
       " 'dnd',\n",
       " 'dng',\n",
       " 'ALBANIA',\n",
       " 'pegging',\n",
       " 'Saudivillie',\n",
       " 'boxwith',\n",
       " 'Pupkin',\n",
       " 'Ooohhhhhh',\n",
       " 'Lovingkindness',\n",
       " 'prospector',\n",
       " 'MEMBER',\n",
       " 'heroically',\n",
       " 'Vernon',\n",
       " 'Timmonds',\n",
       " 'sparrowfart',\n",
       " 'Bighorn',\n",
       " 'mystic',\n",
       " 'DUISA',\n",
       " 'sermons',\n",
       " 'KTOK',\n",
       " 'Pedophilic',\n",
       " 'calcified',\n",
       " 'populations',\n",
       " 'Cheney',\n",
       " 'Senzaman',\n",
       " 'ZIP',\n",
       " 'yahoo',\n",
       " 'meteorologist',\n",
       " 'Penitent',\n",
       " 'SNEEZING',\n",
       " 'cau',\n",
       " 'Strobe',\n",
       " 'symbologist',\n",
       " 'Stentz',\n",
       " 'dialups',\n",
       " 'Starfreighter',\n",
       " 'AIRHORNS',\n",
       " 'blustering',\n",
       " 'listenin',\n",
       " 'obligeant',\n",
       " 'intake',\n",
       " 'morally',\n",
       " 'Ailment',\n",
       " 'Mista',\n",
       " 'JOESo',\n",
       " 'Parrots',\n",
       " 'generalis',\n",
       " 'blaresA',\n",
       " 'ofmyself',\n",
       " 'Lithographs',\n",
       " 'RENDERING',\n",
       " 'EXPERIMENTING',\n",
       " 'Indeedy',\n",
       " 'vowelsAnd',\n",
       " 'hystericallyPrince',\n",
       " 'friandise',\n",
       " 'Sobranies',\n",
       " 'frederics',\n",
       " 'phane',\n",
       " 'Rutaganda',\n",
       " 'decidedSo',\n",
       " '51INT',\n",
       " 'wang',\n",
       " 'wand',\n",
       " 'wane',\n",
       " 'sawingOnce',\n",
       " 'wank',\n",
       " 'Andalousie',\n",
       " 'bottleAn',\n",
       " 'Revive',\n",
       " 'Gatesville',\n",
       " 'titanium',\n",
       " 'Suckered',\n",
       " 'want',\n",
       " 'URINAL',\n",
       " 'rayon',\n",
       " 'Oberon',\n",
       " 'cocksucker',\n",
       " 'obligeance',\n",
       " 'Rissman',\n",
       " 'sugarless',\n",
       " 'CONNS',\n",
       " 'travel',\n",
       " 'Attention',\n",
       " 'Notepad',\n",
       " 'copious',\n",
       " 'airlocks',\n",
       " 'labeledIs',\n",
       " 'piranha',\n",
       " 'wheelAs',\n",
       " 'OLSEN',\n",
       " 'SURGERY',\n",
       " 'PhotographerYou',\n",
       " 'brassard',\n",
       " 'testsThere',\n",
       " 'Thompsons\\xc3',\n",
       " 'HEAD',\n",
       " 'Correspondent',\n",
       " 'Bates',\n",
       " 'goober',\n",
       " 'OWL',\n",
       " 'assimilated',\n",
       " 'HEAL',\n",
       " 'TERM',\n",
       " 'GILBERTSON',\n",
       " 'dinosaurs',\n",
       " 'wrong',\n",
       " 'Fogg',\n",
       " 'HEAR',\n",
       " 'HEAP',\n",
       " 'sentencing',\n",
       " 'Tenants',\n",
       " 'Tiny',\n",
       " 'CFO',\n",
       " 'Stickman',\n",
       " 'mellifluous',\n",
       " 'recombination',\n",
       " 'CFE',\n",
       " 'hairwad',\n",
       " 'Yearning',\n",
       " 'THERAPIST',\n",
       " 'KILLICK',\n",
       " 'LAGOON',\n",
       " 'TRUCKER',\n",
       " 'Shimon',\n",
       " 'CARTOON',\n",
       " 'Retention',\n",
       " 'HiP',\n",
       " 'hophead',\n",
       " 'HESITATES',\n",
       " 'snugly',\n",
       " 'welcomed',\n",
       " 'Blackstone',\n",
       " 'listlessness',\n",
       " 'stoicism',\n",
       " 'DraftGinger',\n",
       " 'REMORSE',\n",
       " 'airbags',\n",
       " 'IUas',\n",
       " 'foreal',\n",
       " 'shmoveryou',\n",
       " 'Robertson',\n",
       " 'ichael',\n",
       " 'activating',\n",
       " 'GARDENS',\n",
       " 'bitterer',\n",
       " 'welcomes',\n",
       " 'fir',\n",
       " 'Evolve',\n",
       " 'Hir',\n",
       " 'His',\n",
       " 'Hit',\n",
       " 'fit',\n",
       " 'lifeline',\n",
       " 'bringing',\n",
       " 'fix',\n",
       " 'BELDAM',\n",
       " 'scrubsink',\n",
       " 'Smiths',\n",
       " 'encrier',\n",
       " 'Montague',\n",
       " 'Vaguely',\n",
       " 'Hic',\n",
       " 'Glimpses',\n",
       " 'fig',\n",
       " 'fie',\n",
       " 'ampule',\n",
       " 'fih',\n",
       " 'RECRUITER',\n",
       " 'fin',\n",
       " 'Him',\n",
       " 'fil',\n",
       " 'Goshen',\n",
       " 'MENSA',\n",
       " 'foolin',\n",
       " 'songwriter',\n",
       " 'salcicc',\n",
       " 'tresspassing',\n",
       " 'Specializes',\n",
       " 'Dingaling',\n",
       " 'Aloo',\n",
       " 'Leniency',\n",
       " 'whatTell',\n",
       " 'effects',\n",
       " 'Alot',\n",
       " 'Gladly',\n",
       " 'LOSING',\n",
       " 'WARNED',\n",
       " 'deliherately',\n",
       " 'Phantoms',\n",
       " 'Freddddddddy',\n",
       " 'Macit',\n",
       " 'meningitus',\n",
       " 'fannies',\n",
       " 'TOChrist',\n",
       " 'Yeeeeaaah',\n",
       " 'SADDLE',\n",
       " 'whacking',\n",
       " 'DeBeers',\n",
       " 'seamstress',\n",
       " 'stopMastrionotti',\n",
       " 'barton',\n",
       " 'sayscocks',\n",
       " 'disconnection',\n",
       " 'reimburse',\n",
       " 'timeout',\n",
       " 'WritersDude',\n",
       " 'gaudiest',\n",
       " 'SELFRIDGE',\n",
       " 'targetthat',\n",
       " 'positionn\\xc3',\n",
       " 'rentes',\n",
       " '7rnn',\n",
       " 'OBJECTING',\n",
       " 'Woodly',\n",
       " 'Combing',\n",
       " 'WEThirties',\n",
       " 'ANNOUNCER',\n",
       " 'ANNOUNCES',\n",
       " 'STRUMMINGHe',\n",
       " 'Evey',\n",
       " 'TERESA',\n",
       " 'sya',\n",
       " 'Cavalcanti',\n",
       " 'parasites',\n",
       " 'lactic',\n",
       " 'insomniacis',\n",
       " 'fi\\xc4',\n",
       " 'nettlod',\n",
       " 'neuroses',\n",
       " 'uneaseShut',\n",
       " 'readsTake',\n",
       " 'Crowds',\n",
       " 'knowOh',\n",
       " 'Smith\\xc3',\n",
       " 'buttercups',\n",
       " 'DIGITIZED',\n",
       " 'golem',\n",
       " 'Susie',\n",
       " 'Tatsi',\n",
       " 'Benedict238',\n",
       " 'chokers',\n",
       " 'indiscretion',\n",
       " 'countenanceStay',\n",
       " 'Joadsesone',\n",
       " 'Unveil',\n",
       " 'XXXL',\n",
       " 'NEEDLES',\n",
       " 'Janessa',\n",
       " 'adapt',\n",
       " 'Basement',\n",
       " 'Spreg',\n",
       " 'Nige1',\n",
       " 'Spree',\n",
       " 'guitarists',\n",
       " 'Chivas',\n",
       " 'STROBE',\n",
       " 'nightdress',\n",
       " 'abbott',\n",
       " 'teMais',\n",
       " 'Callagur',\n",
       " 'MEDIA',\n",
       " 'MEDIC',\n",
       " 'querillas',\n",
       " 'Schwarz',\n",
       " 'magickal',\n",
       " 'POOJA',\n",
       " 'Monaghan',\n",
       " 'pumpkins',\n",
       " 'Ego',\n",
       " 'sym',\n",
       " 'positionne',\n",
       " 'Phyliss',\n",
       " 'estimate',\n",
       " 'Egg',\n",
       " 'Hovitos',\n",
       " 'chlorine',\n",
       " 'silent',\n",
       " 'TELEPORT',\n",
       " 'Nigel',\n",
       " 'sickeningly',\n",
       " 'Cahir',\n",
       " 'NITEINT',\n",
       " 'categorical',\n",
       " 'singsong',\n",
       " 'absolved',\n",
       " 'SIGNED',\n",
       " 'WATCHER',\n",
       " 'ANTHONY',\n",
       " 'subtitledAre',\n",
       " 'Pissing',\n",
       " 'disturbed',\n",
       " 'BAMBAM',\n",
       " 'Facts',\n",
       " 'WATCHED',\n",
       " 'Niger',\n",
       " 'woodey',\n",
       " 'yester',\n",
       " 'retaliationA',\n",
       " 'IMPRESSED',\n",
       " 'breed',\n",
       " 'hurter',\n",
       " 'curiouser',\n",
       " 'Chante',\n",
       " 'slayin',\n",
       " 'Varadero',\n",
       " 'Tino',\n",
       " 'Activity',\n",
       " 'Gingerly',\n",
       " 'Mescalitos',\n",
       " 'FOOTI',\n",
       " 'folderSo',\n",
       " 'SILLY',\n",
       " 'IMPRESSES',\n",
       " 'feldspar',\n",
       " 'AfjQrd',\n",
       " 'saut\\xc3',\n",
       " 'Rioting',\n",
       " 'SLEWS',\n",
       " 'relaxation\\xc2',\n",
       " 'attachedFRANK',\n",
       " 'LANG',\n",
       " 'rett',\n",
       " 'olds',\n",
       " 'LAND',\n",
       " 'renovated',\n",
       " 'LANA',\n",
       " 'BANANA',\n",
       " 'stillHow',\n",
       " 'nerviously',\n",
       " 'Laawng',\n",
       " 'gaskets',\n",
       " 'URINE',\n",
       " 'crueltythe',\n",
       " 'malutki',\n",
       " 'VORACIOUS',\n",
       " 'needed',\n",
       " 'smight',\n",
       " 'streetsYou',\n",
       " 'master',\n",
       " 'understan',\n",
       " 'ragione',\n",
       " 'hiya',\n",
       " 'Huntingburg',\n",
       " 'genesis',\n",
       " 'DRAWNLISTEN',\n",
       " 'toit',\n",
       " 'Hammerstein',\n",
       " 'rewards',\n",
       " 'enthrall',\n",
       " 'Chthonian',\n",
       " 'slingin',\n",
       " 'Fingerprint',\n",
       " 'slavemasters',\n",
       " 'yourselvesI',\n",
       " 'tcugh',\n",
       " 'asylums',\n",
       " 'mutilated',\n",
       " 'Zordon',\n",
       " 'OFFENSE',\n",
       " 'Daddy',\n",
       " 'againHey',\n",
       " 'darkies',\n",
       " 'positively',\n",
       " 'Guardsmen',\n",
       " 'A51',\n",
       " 'SInger',\n",
       " 'SPAN',\n",
       " 'victimFLASH',\n",
       " 'SPAM',\n",
       " 'MOSEYS',\n",
       " 'PCBs',\n",
       " 'grandmama',\n",
       " 'anniversaries',\n",
       " 'SOCK',\n",
       " 'Lanois',\n",
       " 'Rangavertz',\n",
       " 'RACKS',\n",
       " 'exclaimed',\n",
       " 'Calmez',\n",
       " 'CRYING',\n",
       " 'feelink',\n",
       " 'raust',\n",
       " 'sooth',\n",
       " 'shampoos',\n",
       " 'feeling',\n",
       " 'SPEED',\n",
       " 'NEGASCOTT',\n",
       " 'Chicago',\n",
       " 'Tipoca',\n",
       " 'SCHINDLER',\n",
       " 'Recruiting',\n",
       " 'teuse',\n",
       " 'MinistryI',\n",
       " 'consenting',\n",
       " 'atOn',\n",
       " 'peacoat',\n",
       " 'brawny',\n",
       " '279',\n",
       " 'Antonucci',\n",
       " 'Tova',\n",
       " 'Capponi',\n",
       " 'saidGentlemen',\n",
       " 'Theo',\n",
       " 'Then',\n",
       " 'Them',\n",
       " 'Thel',\n",
       " 'wholesome',\n",
       " 'Britney',\n",
       " 'hymen',\n",
       " 'paramedics',\n",
       " 'Thee',\n",
       " 'resumelike',\n",
       " 'Myron',\n",
       " 'Blandly',\n",
       " 'conduction',\n",
       " 'Corroded',\n",
       " 'They',\n",
       " 'affaire',\n",
       " 'Fusco',\n",
       " 'HUMMING',\n",
       " 'Dothan',\n",
       " 'Ther',\n",
       " 'Yokohama',\n",
       " 'shipments',\n",
       " '45th',\n",
       " 'Thumps',\n",
       " 'memebers',\n",
       " 'CarrierWhy',\n",
       " 'Heaving',\n",
       " 'diminishing',\n",
       " 'cinematic',\n",
       " 'resonates',\n",
       " 'Pooja',\n",
       " 'simplify',\n",
       " 'nowhis',\n",
       " 'Pandora',\n",
       " 'Couriers',\n",
       " 'Ferrying',\n",
       " 'dailey',\n",
       " 'rocketplane',\n",
       " 'Encore',\n",
       " 'feelin\\xc3',\n",
       " 'su\\xc4',\n",
       " 'treize',\n",
       " '\\x98.',\n",
       " 'kiya',\n",
       " 'tect',\n",
       " 'Fieldstone',\n",
       " 'longtime',\n",
       " 'SHERIFFRIO',\n",
       " 'PARCEL',\n",
       " 'Civilized',\n",
       " 'tech',\n",
       " 'fugitives',\n",
       " 'superheroesand',\n",
       " 'Hyperdrive',\n",
       " 'itBut',\n",
       " 'ROSOMORF',\n",
       " 'purged',\n",
       " 'Brentford',\n",
       " 'LIFESIGNS',\n",
       " 'saying',\n",
       " 'Involuntarily',\n",
       " 'larded',\n",
       " 'essorer',\n",
       " 'collegeI',\n",
       " 'twoThis',\n",
       " 'Antonioni',\n",
       " 'dicker',\n",
       " 'MAWe',\n",
       " 'Jacobson',\n",
       " 'padded',\n",
       " 'MAINFRAME',\n",
       " 'Bismol',\n",
       " 'Butterfinger',\n",
       " 'Cunha',\n",
       " 'tromps',\n",
       " 'Wisterol',\n",
       " 'tempted',\n",
       " 'Daryl',\n",
       " 'Civilisation',\n",
       " 'hounded',\n",
       " 'VENZABut',\n",
       " 'toobuy',\n",
       " 'apace',\n",
       " '21Can',\n",
       " 'bullshitted',\n",
       " 'clicked',\n",
       " 'PURRING',\n",
       " 'revealSteps',\n",
       " 'vetoing',\n",
       " 'pahtnah',\n",
       " 'restaur',\n",
       " 'Ammonium',\n",
       " 'seesBut',\n",
       " 'Meinhoff',\n",
       " 'UPINTERESTED',\n",
       " 'pustules',\n",
       " 'fingersRemember',\n",
       " 'bullshitter',\n",
       " 'Khomaniacs',\n",
       " 'seriously\\xc3',\n",
       " 'Rigor',\n",
       " 'TEACH',\n",
       " 'Tearing',\n",
       " 'hoeing',\n",
       " 'Braindead',\n",
       " 'SCRUMMING',\n",
       " 'plate',\n",
       " 'TRICIA',\n",
       " 'venez',\n",
       " 'plata',\n",
       " 'Energize',\n",
       " 'Bergin',\n",
       " 'outfielders',\n",
       " 'Hashimi',\n",
       " 'Dionne',\n",
       " 'iate',\n",
       " '7565',\n",
       " 'clumsiness',\n",
       " 'altogether',\n",
       " 'slidin',\n",
       " 'FLORENCE',\n",
       " 'REAPPEARS',\n",
       " 'SPENCE',\n",
       " 'droning',\n",
       " 'Senators',\n",
       " 'Ecology',\n",
       " 'jaguar',\n",
       " 'Fascinating',\n",
       " 'nicely',\n",
       " 'Jabs',\n",
       " 'ZEE',\n",
       " 'Boyleston',\n",
       " 'Kyalami',\n",
       " 'Andover',\n",
       " 'patch',\n",
       " 'Dances',\n",
       " 'Dancer',\n",
       " 'rancho',\n",
       " 'MEYJES',\n",
       " 'pipers',\n",
       " 'nucleocapsids',\n",
       " '10mm',\n",
       " 'knifewhat',\n",
       " 'circling',\n",
       " 'Katie',\n",
       " 'heirloom',\n",
       " 'passesHi',\n",
       " 'Welltell',\n",
       " 'MUNOZ',\n",
       " 'pinon',\n",
       " 'tambi\\xc3',\n",
       " 'nocheIs',\n",
       " 'pinot',\n",
       " 'Anderson',\n",
       " 'Bellissima',\n",
       " 'Logging',\n",
       " 'Bellissimo',\n",
       " 'widges',\n",
       " 'tromp\\xc4',\n",
       " 'Fastidiousness',\n",
       " 'Diplodiaclect',\n",
       " 'nateur',\n",
       " 'Thunderstruck',\n",
       " 'lots',\n",
       " 'movendi',\n",
       " 'clin\\xc3',\n",
       " 'ROBES',\n",
       " 'distanceher',\n",
       " 'likeNo',\n",
       " 'Escrow',\n",
       " 'jubilantly',\n",
       " 'tippin',\n",
       " 'irk',\n",
       " 'Wigs',\n",
       " 'zlotys',\n",
       " 'conductive',\n",
       " 'ira',\n",
       " 'lota',\n",
       " 'ROBED',\n",
       " 'PAINTING',\n",
       " 'ire',\n",
       " 'Adolphus',\n",
       " 'wage',\n",
       " 'redistricting',\n",
       " 'Speculators',\n",
       " 'extend',\n",
       " 'nature',\n",
       " 'perforations',\n",
       " 'Sportman',\n",
       " 'optimist',\n",
       " 'lapping',\n",
       " 'Icos',\n",
       " 'vestment',\n",
       " 'Sussman',\n",
       " 'extent',\n",
       " 'BOYARD',\n",
       " 'tendons',\n",
       " 'boardwalkIt',\n",
       " 'Theory',\n",
       " 'Manischewitz',\n",
       " 'casewhat',\n",
       " 'Sanji',\n",
       " 'Resolute',\n",
       " 'Busik',\n",
       " 'lookit',\n",
       " 'CRUEL',\n",
       " 'ravenously',\n",
       " 'Koontash',\n",
       " 'levitating',\n",
       " 'CORDITE',\n",
       " 'lookin',\n",
       " 'UPTHOU',\n",
       " 'Bertolucci',\n",
       " 'Pebble',\n",
       " 'Tomassino',\n",
       " 'lookie',\n",
       " 'fearlessly',\n",
       " 'lookig',\n",
       " 'ANTONIO',\n",
       " 'UNHARMED',\n",
       " 'Apartment',\n",
       " 'numbnut',\n",
       " 'fondles',\n",
       " 'GOWN',\n",
       " 'gopher',\n",
       " 'HNN',\n",
       " 'fondled',\n",
       " 'minuses',\n",
       " 'thatGot',\n",
       " 'ANVRIN',\n",
       " 'kidI',\n",
       " 'escalade',\n",
       " 'surname',\n",
       " 'humming',\n",
       " 'orderDewar',\n",
       " 'brainwashes',\n",
       " 'Isotonic',\n",
       " 'Instantly',\n",
       " 'underdone',\n",
       " 'AAAAAAA',\n",
       " 'Nidre',\n",
       " 'fre',\n",
       " 'amdist',\n",
       " 'Lethe',\n",
       " 'fro',\n",
       " 'Diathermic',\n",
       " '.',\n",
       " 'muck',\n",
       " 'Busies',\n",
       " 'much',\n",
       " 'Cucumber',\n",
       " 'chink',\n",
       " 'Closely',\n",
       " 'deliberately',\n",
       " 'fry',\n",
       " 'WritersBabe',\n",
       " 'tutoyer',\n",
       " 'BELTED',\n",
       " 'Snake',\n",
       " 'obese',\n",
       " 'Katsumoto',\n",
       " 'spit',\n",
       " 'barque',\n",
       " 'Owchee',\n",
       " 'Asael',\n",
       " 'Rudys',\n",
       " 'backthe',\n",
       " 'Clays',\n",
       " 'spic',\n",
       " 'Donovan',\n",
       " 'DeVito',\n",
       " 'Pricks',\n",
       " 'doubts',\n",
       " 'DISTRACTEDLY',\n",
       " 'automati',\n",
       " 'spin',\n",
       " 'skilfully',\n",
       " 'wildcat',\n",
       " 'ROTATE',\n",
       " 'Piken',\n",
       " 'DIELLO',\n",
       " 'preempting',\n",
       " 'voudra',\n",
       " 'contingencies',\n",
       " 'intah',\n",
       " 'professionally',\n",
       " 'Meckers',\n",
       " 'Toothpaste',\n",
       " 'misconstrued',\n",
       " 'droppin',\n",
       " '1672',\n",
       " 'orgasmic\\xc4',\n",
       " 'forwarder',\n",
       " 'prostrate',\n",
       " 'Crusaders',\n",
       " 'biddies',\n",
       " 'fr\\xc3',\n",
       " 'aboyer',\n",
       " 'cupful',\n",
       " 'Nausea',\n",
       " 'canoeing',\n",
       " 'fr\\xc4',\n",
       " 'riparian',\n",
       " 'Testarosa',\n",
       " 'Cuidad',\n",
       " 'cacophonous',\n",
       " '5Gotta',\n",
       " 'Rothgar',\n",
       " 'conditioned',\n",
       " 'Carmes',\n",
       " 'Sommers',\n",
       " 'DUBIOUS',\n",
       " 'Sobre',\n",
       " 'includeAs',\n",
       " 'hyperdrive',\n",
       " 'conditioner',\n",
       " 'Embossed',\n",
       " 'hone',\n",
       " 'circumferences',\n",
       " 'memorial',\n",
       " 'SCHAEFFER',\n",
       " 'Vagrancy',\n",
       " 'rightwingers',\n",
       " 'fluffer',\n",
       " 'mummified',\n",
       " 'youhaveto',\n",
       " 'honk',\n",
       " 'Tahitian',\n",
       " 'spews',\n",
       " 'modivation',\n",
       " 'cherubim',\n",
       " 'BARBERSHOP',\n",
       " 'TANAXA',\n",
       " 'visitations',\n",
       " 'provincials',\n",
       " 'Swayze',\n",
       " 'Married',\n",
       " 'flipflops',\n",
       " 'myocardial',\n",
       " 'chauvinistic',\n",
       " 'Maybe',\n",
       " 'Isley',\n",
       " 'torpedoes',\n",
       " 'CfJI3',\n",
       " 'ozhidala',\n",
       " 'NATION',\n",
       " 'BILLBOARD',\n",
       " 'Imagined',\n",
       " 'ABASHED',\n",
       " 'SPITTING',\n",
       " 'bringin\\xc2',\n",
       " 'RYKO',\n",
       " 'torpedoed',\n",
       " 'effortEmpties',\n",
       " 'Hirschbergs',\n",
       " 'pigglies',\n",
       " 'yellsHey',\n",
       " '!!!!!!!!',\n",
       " 'seePut',\n",
       " 'Bludvist',\n",
       " 'furlough',\n",
       " 'McNeely',\n",
       " 'SEYMOUR',\n",
       " 'venant',\n",
       " 'asWe',\n",
       " 'Verger',\n",
       " 'darkside',\n",
       " 'Isles',\n",
       " 'academic',\n",
       " 'stillness',\n",
       " 'academia',\n",
       " 'lonelier',\n",
       " 'SCREECHES',\n",
       " 'nomore',\n",
       " 'Table',\n",
       " 'corporate',\n",
       " 'massaging',\n",
       " 'CLASSROOM',\n",
       " '???.',\n",
       " '???-',\n",
       " 'bellow',\n",
       " 'Promoted',\n",
       " 'absurdities',\n",
       " 'golden',\n",
       " 'CHINOIS',\n",
       " 'Awed',\n",
       " 'Lothlorien',\n",
       " 'groinecologist',\n",
       " 'Knievel',\n",
       " '????',\n",
       " 'AFFECTIONATELY',\n",
       " 'promets',\n",
       " 'PRIMITIVES',\n",
       " 'lassi',\n",
       " 'lasso',\n",
       " 'hah',\n",
       " 'hai',\n",
       " 'hal',\n",
       " 'ham',\n",
       " 'han',\n",
       " 'Sportsmanship',\n",
       " 'Oscar',\n",
       " 'hab',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter the words based on the unigram count in the files\n",
    "# Create the vocab file\n",
    "# Filter the embeddings to only include the words in vocab\n",
    "# Create fake embedding for _PAD, UNK, _GO, _EOS add them at start of vocab"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
